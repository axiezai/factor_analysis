{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from matplotlib.lines import Line2D\n",
    "from IPython.display import HTML\n",
    "\n",
    "def loadmat(filename):\n",
    "    '''\n",
    "    this function should be called instead of direct spio.loadmat\n",
    "    as it cures the problem of not properly recovering python dictionaries\n",
    "    from mat files. It calls the function check keys to cure all entries\n",
    "    which are still mat-objects\n",
    "    '''\n",
    "    data = spio.loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
    "    return _check_keys(data)\n",
    "\n",
    "def _check_keys(dict):\n",
    "    '''\n",
    "    checks if entries in dictionary are mat-objects. If yes\n",
    "    todict is called to change them to nested dictionaries\n",
    "    '''\n",
    "    for key in dict:\n",
    "        if isinstance(dict[key], spio.matlab.mio5_params.mat_struct):\n",
    "            dict[key] = _todict(dict[key])\n",
    "    return dict        \n",
    "\n",
    "def _todict(matobj):\n",
    "    '''\n",
    "    A recursive function which constructs from matobjects nested dictionaries\n",
    "    '''\n",
    "    dict = {}\n",
    "    for strg in matobj._fieldnames:\n",
    "        elem = matobj.__dict__[strg]\n",
    "        if isinstance(elem, spio.matlab.mio5_params.mat_struct):\n",
    "            dict[strg] = _todict(elem)\n",
    "        else:\n",
    "            dict[strg] = elem\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Bayes Expectation Maximization (VB-EM algorithm)\n",
    "---\n",
    "\n",
    "Solve the factor analysis model: $$y = A x + \\nu$$\n",
    "\n",
    "Where $y$ is the data, $A$ is the mixing matrix (factor loading matrix), $x$ is the unknown, and $\\nu$ is the noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data first, this data is an auditory evoked field (AEF) recording provided by Chang and Dr. John Houde's group. \n",
    "I am not sure what the actual stimulus was, but the data includes the following:\n",
    "\n",
    "```\n",
    "struct with fields:\n",
    "\n",
    "             filename: 'HoudeJohn_AEF_20060710_01-1grad-av'\n",
    "                 data: [720x274 single]\n",
    "          sensorCoord: [274x3x2 double]\n",
    "         sensorOrient: [274x3 double]\n",
    "                srate: 1200\n",
    "              latency: [720x1 double]\n",
    "                  lsc: [1.1200 -1.5000 53.0800]\n",
    "        sensor_labels: {1x274 cell}\n",
    "                Gcoef: [274x29 double]\n",
    "           grad_order: 1\n",
    "       refSensorCoord: [29x3x2 double]\n",
    "      refSensorOrient: [29x3 double]\n",
    "    ref_sensor_labels: {1x29 cell}\n",
    "         goodchannels: [1x274 double]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEG_dict = loadmat('nuts_meg_data.mat')\n",
    "print('The data field contains {0} rows and {1} columns'.format(MEG_dict['nuts']['meg']['data'].shape[0], MEG_dict['nuts']['meg']['data'].shape[1]))\n",
    "print('The sampling rate is {0} times per second'.format(MEG_dict['nuts']['meg']['srate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Visualize:\n",
    "t = np.linspace(0,720,720)/1200\n",
    "plt.plot(t,MEG_dict['nuts']['meg']['data']);\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous class notes, the BFA framework includes the following: \n",
    "\n",
    "$$ y = ax + \\nu $$$ \n",
    "$$$ P(x) = \\prod_{t=1}^{T} \\mathcal{N}\\left(x_{t} | 0, I\\right) $$$\n",
    "$$$ P(\\nu) = \\prod_{t=1}^{T} \\mathcal{N}\\left(\\nu_{t} | 0, \\Lambda\\right) $$$\n",
    "$$$ P(A) = \\prod_{T_j} P(A_{ij} | 0, \\alpha_{ij}) $$\n",
    "\n",
    "where $\\Lambda$ is the noise precision.\n",
    "\n",
    "If all variations in the data come from our latent variables, not A, we can simplify $P(A)$ to $P(A) = \\delta(A-\\hat{A})$. Then the unkowns we have are $A$, $x$, and $\\Lambda$, and $\\Lambda$ is a diagonal noise matrix, making our problem a <em>\"heteroscedastic noise model\"</em>. We aim to compute $p(x|y)$.\n",
    "\n",
    "For expectation-maximization, we want to \n",
    " - 1) Compute $P(x|y)$\n",
    " - 2) Compute $\\mathbb{E} P(x|y) [log P(y,x)]$\n",
    " - 3) Maximize $\\mathbb{E} P(x|y) [log P(y,x)]$\n",
    " \n",
    "For step 1 - We know $log P(x|y) = log P(y|x) + log P(x) + constant$, and if we expand this equation out we can find $\\bar{x}$ and $\\Gamma$, which comes from the first and second order derivatives of $L$ respectively.\n",
    "\n",
    "For step 2 - We make use of Baye's rule, the joint distribution $P(y,x)$ is $P(y|x)P(x)$, so $\\mathbb{E}_{P(x|y)}[log P(y,x)] = \\mathbb{E}_{P(x|y)}{L}$\n",
    "\n",
    "For step 3 - We need to find $A$ and $\\Lambda$ by taking the derivative of the log-likelihood $\\mathbb{F}$ with respect to $A$ and $\\Lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions to unknowns:\n",
    "\n",
    "$$ \\bar{x} = \\Gamma^{-1} A^{T} \\Lambda y$$\n",
    "$$ \\Gamma = A^T \\Lambda A + I $$\n",
    "$$ A = R_{xx}^{-1} R_{yy}$$\n",
    "$$ \\Lambda^{-1} = diag((y - \\bar{A}\\bar{x})(y -\\bar{A}\\bar{x}^T))$$\n",
    "\n",
    "These solutions make up the update rules that will maximize the log likelihood in the expectation maximization algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the data and compute it's autocorrelation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want rows = Nsensors (274) and columns = Ntimes(720) \n",
    "y = np.transpose(MEG_dict['nuts']['meg']['data'])\n",
    "num_y = y.shape[0] #data number of sensors\n",
    "num_t = y.shape[1] #data time samples\n",
    "\n",
    "# auto-correlation:\n",
    "autocorr_yy = np.matmul(y,np.transpose(y))\n",
    "\n",
    "# visualize:\n",
    "plt.imshow(autocorr_yy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize by singular value decomposition and compute the mixing matrix and diagonal noise precision matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_it = 20 # number of EM-iterations\n",
    "num_fa = 5  # number of factors used \n",
    "\n",
    "U, SV, V = np.linalg.svd(autocorr_yy/num_t)\n",
    "D = np.diag(SV)\n",
    "\n",
    "mixing_matrix = U * np.diag(np.sqrt(D))\n",
    "mixing_matrix = mixing_matrix[:,0:num_fa]\n",
    "noise_precision_matrix = np.diag(num_t/np.diag(autocorr_yy))\n",
    "\n",
    "# EM iteration:\n",
    "likelihood = np.zeros([num_it, 1])\n",
    "lambda_psi = np.matmul(np.matmul(np.transpose(mixing_matrix),noise_precision_matrix), mixing_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, (ax_1, ax_2, ax_3) = plt.subplots(1,3, figsize = (12,9))\n",
    "ax_1.imshow(mixing_matrix, extent = (-0.5, 4.5, -0.5, 4.5))\n",
    "ax_1.set_title('Mixing Matrix')\n",
    "ax_2.imshow(noise_precision_matrix)\n",
    "ax_2.set_title('Noise Precision Matrix')\n",
    "ax_3.imshow(lambda_psi)\n",
    "ax_3.set_title('LambdaPsi')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = lambda_psi + np.eye(num_fa) #precision\n",
    "im = plt.imshow(gamma); plt.colorbar(im);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "igamma = np.linalg.inv(gamma) # variance\n",
    "im2 = plt.imshow(igamma); plt.colorbar(im2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize y, ybar, xbar side by side\n",
    "xbar = np.matmul(np.matmul(np.matmul(igamma, np.transpose(mixing_matrix)),noise_precision_matrix), y)\n",
    "ybar = np.matmul(mixing_matrix,xbar)\n",
    "fig, ax = plt.subplots(1,3, figsize = (15, 5))\n",
    "ax[0].plot(t,y.transpose());\n",
    "ax[1].plot(t,ybar.transpose());\n",
    "ax[2].plot(t,xbar.transpose());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig2 = plt.figure(figsize = (15,12))\n",
    "grid = plt.GridSpec(2, 3, wspace=0.4, hspace=0.3)\n",
    "lh_ax = fig2.add_subplot(grid[0, 0])\n",
    "np_ax = fig2.add_subplot(grid[0, 1])\n",
    "ig_ax = fig2.add_subplot(grid[0, 2])\n",
    "xb_ax = fig2.add_subplot(grid[1,0:])\n",
    "\n",
    "for i in np.arange(0, num_it):\n",
    "    gamma = lambda_psi + np.eye(num_fa)\n",
    "    igamma = np.linalg.inv(gamma)\n",
    "    xbar = np.matmul(np.matmul(np.matmul(igamma, np.transpose(mixing_matrix)),noise_precision_matrix), y)\n",
    "\n",
    "    # update rules:\n",
    "    ldnpm = np.sum(np.log(np.diag(noise_precision_matrix/(2*np.pi))))\n",
    "    ldgam = np.sum(np.log(np.linalg.svd(gamma)[1]))\n",
    "    likelihood[i] = 0.5*num_t*(ldnpm - ldgam) - 0.5*np.sum(np.sum(y*np.matmul(noise_precision_matrix,y))) + 0.5*np.sum(np.sum(xbar*np.matmul(gamma,xbar)))\n",
    "    \n",
    "    # update:\n",
    "    ryx = np.matmul(y,xbar.transpose())\n",
    "    rxx = np.matmul(xbar,xbar.transpose()) + num_t*igamma\n",
    "    psi = np.linalg.inv(rxx)\n",
    "    mixing_matrix = np.matmul(ryx,psi)\n",
    "    noise_precision_matrix = np.diag(num_t/np.diag(autocorr_yy - np.matmul(mixing_matrix, ryx.transpose())))\n",
    "    lambda_psi = np.matmul(mixing_matrix.transpose(), np.matmul(noise_precision_matrix, mixing_matrix))\n",
    "        \n",
    "    # visualize updates:\n",
    "    lh_ax.clear()\n",
    "    np_ax.clear()\n",
    "    xb_ax.clear()\n",
    "    ig_ax.clear()\n",
    "    lh_ax.plot(np.arange(0,i),likelihood[:i])\n",
    "    lh_ax.set_title('Likelihood')\n",
    "    np_ax.plot(1/np.diag(noise_precision_matrix))\n",
    "    np_ax.set_title('Noise Precision')\n",
    "    ig_ax.imshow(igamma)\n",
    "    ig_ax.set_title('Variance')\n",
    "    xb_ax.plot(t, xbar.transpose())\n",
    "    xb_ax.set_title('iteration {}'.format(i+1))\n",
    "    fig2.canvas.draw()\n",
    "    fig2.canvas.flush_events()\n",
    "    plt.show()\n",
    "    plt.pause(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for vb-fa:\n",
    "def vbfa(y, num_fa, num_it, mixing_matrix = None, noise_precision = None):\n",
    "    num_y = y.shape[0] #data number of sensors\n",
    "    num_t = y.shape[1] #data time samples\n",
    "    # set up plotting grid:\n",
    "    fig_em = plt.figure(figsize = (15,12))\n",
    "    grid = plt.GridSpec(2, 3, wspace=0.4, hspace=0.3)\n",
    "    lh_ax = fig_em.add_subplot(grid[0, 0])\n",
    "    np_ax = fig_em.add_subplot(grid[0, 1])\n",
    "    ig_ax = fig_em.add_subplot(grid[0, 2])\n",
    "    xb_ax = fig_em.add_subplot(grid[1,0:])\n",
    "    # auto-correlation:\n",
    "    ryy = np.matmul(y,np.transpose(y))\n",
    "    \n",
    "    # initialize SVD:\n",
    "    if mixing_matrix is None:\n",
    "        U, SV, V = np.linalg.svd(ryy/num_t)\n",
    "        D = np.diag(SV)\n",
    "        mixing_matrix = U * np.diag(np.sqrt(D))\n",
    "        mixing_matrix = mixing_matrix[:,0:num_fa]\n",
    "        noise_precision_matrix = np.diag(num_t/np.diag(ryy))\n",
    "    elif noise_precision is None:\n",
    "        mixing_matrix = mixing_matrix\n",
    "        noise_precision_matrix = np.diag(num_t/np.diag(ryy))\n",
    "    \n",
    "    # EM iteration:\n",
    "    likelihood = np.zeros([num_it, 1])\n",
    "    lambda_psi = np.matmul(np.matmul(np.transpose(mixing_matrix),noise_precision_matrix), mixing_matrix)\n",
    "\n",
    "    for i in np.arange(0, num_it):\n",
    "        gamma = lambda_psi + np.eye(num_fa)\n",
    "        igamma = np.linalg.inv(gamma)\n",
    "        xbar = np.matmul(np.matmul(np.matmul(igamma, np.transpose(mixing_matrix)),noise_precision_matrix), y)\n",
    "\n",
    "        # update rules:\n",
    "        ldnpm = np.sum(np.log(np.diag(noise_precision_matrix/(2*np.pi))))\n",
    "        ldgam = np.sum(np.log(np.linalg.svd(gamma)[1]))\n",
    "        likelihood[i] = 0.5*num_t*(ldnpm - ldgam) - 0.5*np.sum(np.sum(y*np.matmul(noise_precision_matrix,y))) + 0.5*np.sum(np.sum(xbar*np.matmul(gamma,xbar)))\n",
    "\n",
    "        # update:\n",
    "        ryx = np.matmul(y,xbar.transpose())\n",
    "        rxx = np.matmul(xbar,xbar.transpose()) + num_t*igamma\n",
    "        psi = np.linalg.inv(rxx)\n",
    "        mixing_matrix = np.matmul(ryx,psi)\n",
    "        noise_precision_matrix = np.diag(num_t/np.diag(ryy - np.matmul(mixing_matrix, ryx.transpose())))\n",
    "        lambda_psi = np.matmul(mixing_matrix.transpose(), np.matmul(noise_precision_matrix, mixing_matrix))\n",
    "        \n",
    "        # visualize updates:\n",
    "        lh_ax.clear()\n",
    "        np_ax.clear()\n",
    "        ig_ax.clear()\n",
    "        xb_ax.clear()\n",
    "        lh_ax.plot(np.arange(0,i),likelihood[:i])\n",
    "        lh_ax.set_title('Likelihood')\n",
    "        np_ax.plot(1/np.diag(noise_precision_matrix))\n",
    "        np_ax.set_title('Noise Precision')\n",
    "        ig_ax.imshow(igamma)\n",
    "        ig_ax.set_title('Variance')\n",
    "        xb_ax.plot(t, xbar.transpose())\n",
    "        xb_ax.set_title('Iteration {}'.format(i+1))\n",
    "        fig2.canvas.draw()\n",
    "        fig2.canvas.flush_events()\n",
    "        plt.show()\n",
    "        plt.pause(0.5)\n",
    "    return mixing_matrix, noise_precision_matrix, xbar, igamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize with random mixing matrix:\n",
    "num_t = y.shape[1]\n",
    "ryy = np.matmul(y,y.transpose())/num_t #auto-correlation\n",
    "scaling = np.sum(np.diag(np.diag(ryy)))\n",
    "mixing_matrix = np.random.rand(274,5)*scaling #random mixing matrix for initialization\n",
    "\n",
    "a, npm, xbar, igamma = vbfa(y, num_fa = 5, num_it = 50, mixing_matrix = mixing_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
